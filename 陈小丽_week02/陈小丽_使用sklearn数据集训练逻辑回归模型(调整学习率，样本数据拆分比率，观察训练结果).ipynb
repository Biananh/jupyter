{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9fe6ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearning():\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import numpy as np\n",
    "\n",
    "    X,y = make_classification(n_samples=200, n_features=20)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "    theta = np.random.randn(1,20)\n",
    "    bias = 0\n",
    "    lr = 0.1\n",
    "    epochs = 3000\n",
    "\n",
    "    def forward(x, theta, bias):\n",
    "        z = np.dot(theta, x.T) + bias \n",
    "        y_hat = 1 / (1 + np.exp(-z))\n",
    "        return y_hat\n",
    "\n",
    "    def loss(y, y_hat):\n",
    "        e = 1e-8\n",
    "        return - y * np.log(y_hat + e) - (1 - y) * np.log(1 - y_hat + e)\n",
    "\n",
    "\n",
    "    def calc_gradient(x,y,y_hat):\n",
    "        m = x.shape[-1]\n",
    "        delta_theta = np.dot((y_hat - y), x) / m\n",
    "        delta_bias = np.mean(y_hat - y)\n",
    "        return delta_theta, delta_bias\n",
    "\n",
    "    for i in range(epochs):\n",
    "        y_hat = forward(X_train, theta, bias)\n",
    "        loss_val = loss(y_train, y_hat)\n",
    "        delta_theta, delta_bias = calc_gradient(X_train, y_train, y_hat)\n",
    "        theta = theta - lr * delta_theta\n",
    "        bias = bias - lr * delta_bias\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            acc = np.mean(np.round(y_hat) == y_train)  \n",
    "            print(f\"epoch: {i}, loss: {np.mean(loss_val)}, acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "557beedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearning1():\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import numpy as np\n",
    "\n",
    "    X1,y1 = make_classification(n_samples=250, n_features=10)\n",
    "    X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.4)\n",
    "\n",
    "    theta1 = np.random.randn(1,10)\n",
    "    bias1 = 0\n",
    "    lr1 = 0.22\n",
    "    epochs1 = 4000\n",
    "\n",
    "    def forward(x1, theta1, bias1):\n",
    "        z = np.dot(theta1, x1.T) + bias1 \n",
    "        y_hat1 = 1 / (1 + np.exp(-z))\n",
    "        return y_hat1\n",
    "\n",
    "    def loss(y1, y_hat1):\n",
    "        e = 1e-8\n",
    "        return - y1 * np.log(y_hat1 + e) - (1 - y1) * np.log(1 - y_hat1 + e)\n",
    "\n",
    "\n",
    "    def calc_gradient(x1,y1,y_hat1):\n",
    "        m = x1.shape[-1]\n",
    "        delta_theta1 = np.dot((y_hat1 - y1), x1) / m\n",
    "        delta_bias1 = np.mean(y_hat1 - y1)\n",
    "        return delta_theta1, delta_bias1\n",
    "\n",
    "    for i in range(epochs1):\n",
    "        y_hat1 = forward(X1_train, theta1, bias1)\n",
    "        loss_val = loss(y1_train, y_hat1)\n",
    "        delta_theta1, delta_bias1 = calc_gradient(X1_train, y1_train, y_hat1)\n",
    "        theta1 = theta1 - lr1 * delta_theta1\n",
    "        bias1 = bias1 - lr1 * delta_bias1\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            acc = np.mean(np.round(y_hat1) == y1_train)  \n",
    "            print(f\"epoch: {i}, loss: {np.mean(loss_val)}, acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2859590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearning2():\n",
    "    from sklearn.datasets import make_classification\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import numpy as np\n",
    "\n",
    "    X2,y2= make_classification(n_samples=300, n_features=20)\n",
    "    X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.6)\n",
    "\n",
    "    theta2 = np.random.randn(1,20)\n",
    "    bias2 = 0\n",
    "    lr1 = 0.5\n",
    "    epochs2 = 3500\n",
    "\n",
    "    def forward(x2, theta2, bias2):\n",
    "        z = np.dot(theta2, x2.T) + bias2 \n",
    "        y_hat2 = 1 / (1 + np.exp(-z))\n",
    "        return y_hat2\n",
    "\n",
    "    def loss(y2, y_hat2):\n",
    "        e = 1e-8\n",
    "        return - y2 * np.log(y_hat2 + e) - (1 - y2) * np.log(1 - y_hat2 + e)\n",
    "\n",
    "\n",
    "    def calc_gradient(x2,y2,y_hat2):\n",
    "        m = x2.shape[-1]\n",
    "        delta_theta2 = np.dot((y_hat2 - y2), x2) / m\n",
    "        delta_bias2 = np.mean(y_hat2 - y2)\n",
    "        return delta_theta2, delta_bias2\n",
    "\n",
    "    for i in range(epochs2):\n",
    "        y_hat2 = forward(X2_train, theta2, bias2)\n",
    "        loss_val2 = loss(y2_train, y_hat2)\n",
    "        delta_theta2, delta_bias2 = calc_gradient(X2_train, y2_train, y_hat2)\n",
    "        theta2 = theta2 - lr1 * delta_theta2\n",
    "        bias2 = bias2 - lr1 * delta_bias2\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            acc = np.mean(np.round(y_hat2) == y2_train)  \n",
    "            print(f\"epoch: {i}, loss: {np.mean(loss_val)}, acc: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25fc34f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
